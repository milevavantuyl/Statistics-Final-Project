---
title: "forestfire_analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Import packages
```{r}
library(ggplot2)
library(plotly)
library(pheatmap)
library(corrplot)
library(tidyverse)
library(sqldf)
library(ggpubr)
library(rstatix)
library(car)
```

## Load Data
```{r}
fires <- read.csv("forestfires.csv", header = TRUE, sep = ",")
fires$X <- as.factor(fires$X)
fires$Y <- as.factor(fires$Y)
fires.withoutOutliers <- fires[fires$area <= 300,]
# View(fires)
```

## Part 1: Descriptive Analysis

Variables of interest for descriptive analysis. 
Distributions of all the numeric variables. 
Specifically: area, months, and location variation by y value (North/ South)

### Distribution of response variable
```{r}
summary(fires$area)
range(fires$area)
sd(fires$area)
```

```{r}
# Boxplots
ggplot(fires, aes(y = area)) + geom_boxplot()
ggplot(fires, aes(y = area)) + geom_boxplot() + ylim(0,300)
```

# Histogram
```{r}
ggplot(fires, aes(x = area)) + geom_histogram()
ggplot(fires, aes(x = area)) + geom_histogram() + xlim(-1, 300) + ylim(0, 100)
```

TO DO - Explain outliers

### Distribution of categorical explanatory variable (month)
```{r}
fires$month <- factor(fires$month, c('jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'))
monthly.rates <- as.data.frame(table(fires$month))
colnames(monthly.rates) <- c('month', 'count')
ggplot(monthly.rates, aes(x=month, y = count)) + geom_col(color = 'black', fill = 'firebrick3') + theme_classic() + ggtitle("Frequency of fires by month") +
  theme(plot.title = element_text(hjust = 0.5))
```

### Distribution of categorical explanatory variable (region)
TO DO - find better representation of this data

```{r}
regional.rates <- as.data.frame(table(fires$Y))
colnames(regional.rates) <- c('ycoord', 'count')
ggplot(regional.rates, aes(x=ycoord, y = count)) + geom_col(color = 'black', fill = 'firebrick3') + theme_classic() + ggtitle("Frequency of fires by ycoordinate") +
  theme(plot.title = element_text(hjust = 0.5))
```
```{r}
regional.rates <- as.data.frame(table(fires$X, fires$Y))
colnames(regional.rates) <- c('ycoord', 'xcoord', 'count')
ggplot(regional.rates, aes(x=xcoord, y = count, fill = ycoord)) + geom_col() + theme_classic() + ggtitle("Frequency of fires by region") +
  theme(plot.title = element_text(hjust = 0.5))
```
# Distribution of all numeric columns 
```{r}
numeric.cols <- fires %>% select(is.numeric) %>% reshape2::melt()
# ggplot(numeric.cols, aes(y = value)) + geom_boxplot(aes(fill=variable)) + facet_wrap(~variable, scales = "free")
ggplot(numeric.cols, aes(x = value)) + geom_histogram() + facet_wrap(~variable, scales = "free")
```

### TO DO: Look into temporal differences by month

## Part 2: One-sample statistical inference for response variable 

The expected size of a forest fire in Portugal is 3.43 ha per fire (obtained the statistics below from https://www.icnf.pt/)
```{r}
num.fires <- 8181
burned.area <- 28088
(burned.area.per.fire <- burned.area/num.fires)
```

1. Random sample? - No since we're only looking at samples from one forest in Portugal
2. Normality assumption is not satisfied. Right skewed
3. Outliers are present. We should remove them. 

Let $H_0: \mu = 3.433$ and $H_0: \mu \neq 3.433$.

```{r}
t.test(fires$area, mu = 3.433)
# Cohen's d
(mean(fires$area) - 3.433) / sd(fires$area)
```
The p-value is very small (0.000829). The result is statistically significant and so we reject the null hypothesis.   

Cohen's d is 0.148 which is very small, so the result has very small practical significance. 

This corresponds with our observations from the confidence interval. The confidence interval is from 7.34 to 18.34. So the value 3.433 is not in the confidence interval and is thus not statistically significant. Additionally, 3.433 is relatively far from the lower end of the confidence interval and is thus also not practically significant. 

```{r}
# Sample computation after removing outlier
t.test(fires.withoutOutliers$area, mu = 3.433)
# Cohen's d
(mean(fires.withoutOutliers$area) - 3.433) / sd(fires.withoutOutliers$area)
```

## Part 3: Two-sample statistical inference for categorical variable 

## Part 4: Two-sample statistical inference

load data
```{r}
df <- fires
```



group by season
```{r}
#winter spring split
df$WinterSpring <- (df$month == 'mar' | df$month == 'apr' | df$month == 'may' | df$month == 'dec' | df$month == 'jan' | df$month == 'feb' )
#summer fall split
df$SummerFall <- (df$month == 'jun' | df$month == 'jul' | df$month == 'aug' | df$month == 'sep' | df$month == 'oct' | df$month == 'nov' )
#combine
df<-df%>%pivot_longer(-c(1:13), names_to="seasons")%>%filter(value==TRUE)%>%select(-value)
```


make seasons different tables
```{r}
s1 <- sqldf("select * from df where seasons = 'WinterSpring'  and  area < 50")
s2 <- sqldf("select * from df where seasons = 'SummerFall'  and area < 400")
#different outlier split
s2_1 <- sqldf("select * from s2 where seasons = 'SummerFall'  and area < 125")
```

summary of variables
```{r}
summary(s1$area)
summary(s2$area)
ggboxplot(df, x = "seasons", y = "area", 
          color = "seasons", palette = c("#00AFBB", "#E7B800"),
        ylab = "Area", xlab = "Season")
        
boxplot(s1$area,col="#00AFBB",xlab='WinterSpring',ylab='Area')
boxplot(s2$area,col="#E7B800",xlab='SummerFall',ylab='Area')
boxplot(s2_1$area,col="#E7B800",xlab='SummerFall',ylab='Area')
```

assumption tests
```{r}
#normality
with(s1, shapiro.test(area[seasons == 'WinterSpring']))
with(s2, shapiro.test(area[seasons == 'SummerFall']))
#homogeneity
res.ftest <- var.test(area ~ seasons, data = df)
res.ftest
```


t-test
```{r}
ave_wint <- mean(s1$area)
ave_sum <- mean(s2$area)
res <- t.test(s1$area, s2$area)
res
```
cohen's d 
```{r}
cohens_d(df, area ~ seasons, var.equal = FALSE)
```


## Part 5: Which explanatory variables are the best predictors for 'area'?
### Correlation plot of numeric explanatory variables


```{r}
corr_matrix <- fires %>%
                  select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area) %>%
                  cor
corr_matrix
corrplot(corr_matrix, addCoef.col = 'black')
```

The correlation matrix indicated that all of our explanatory variables have very weak associations with area burned. The strongest associations are a positive linear correlation with temperature (0.1), a negative linear correlation with Relative Humidity (RH, -0.08), and a positive linear correlation with Duff Moisture Code (DMC, 0.07) which represents the moisture of materials below the fine fuels.




```{r}
mult_reg <- lm(area ~ FFMC+DMC+DC+ISI+temp+RH+wind+rain, data = fires)
summary(mult_reg)
```

### Interpreting the model

Residuals: The difference between the observed value and the predicted value. The median residual is -9.35, the min -31.13, and the max is 1068.99. The large value of the max residual indicated we may want to remove outliers.

Pr(>|t|): This is the p-value that corresponds to the t-statistic. If this value is less than some alpha level (e.g. 0.05) than the predictor variable is said to be statistically significant. As can be seen, none of our explanatory variables can be said to be statistically significant predictors of area burned, but the least insignificant are DMC (0.253) and temperature (0.282)

Residual standard error: 63.64. The average distance that the observed values fall from the regression line. 

Multiple R-Squared: The coefficient of determination. At only 0.016, it tells us that only 1.6% of the variance in the response variable can be explained by the predictor variables.

p-value: This is the p-value that corresponds to the F-statistic. If this value is less than some significance level (e.g. 0.05), then the regression model fits the data better than a model with no predictors. Again, at 0.4096, our model cannot be said to be statistically significant.


### Added Variable/Partial Regression plots
Added variable plots are individual plots that display the relationship between a response variable and one predictor variable in a multiple linear regression model, while controlling for the presence of other predictor variables in the model.

```{r}
avPlots(mult_reg)
```


### Outliers removed

```{r}
corr_matrix <- fires.withoutOutliers %>%
                  select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area) %>%
                  cor
corr_matrix
corrplot(corr_matrix, addCoef.col = 'black')
```

With outliers removed, the correlations between individual explanatory variables are even weaker.


```{r}
mult_reg_outliers <- lm(area ~ FFMC+DMC+DC+ISI+temp+RH+wind+rain, data = fires.withoutOutliers)
summary(mult_reg_outliers)
avPlots(mult_reg_outliers)
```

We can see that our residuals have gotten smaller after removing outliers, as expected. Unfortunately, our model cannot be said to fit any better, with a slightly smaller coefficient of determination and a larger p-value. Interestingly in this model ISI has become much more significant, with a Pr(>|t|) value of 0.0949.



